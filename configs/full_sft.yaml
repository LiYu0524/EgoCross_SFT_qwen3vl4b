# EgoCross Full SFT Training Configuration
# Usage: FORCE_TORCHRUN=1 NNODES=1 NPROC_PER_NODE=4 llamafactory-cli train configs/full_sft.yaml

# Model
model_name_or_path: Qwen/Qwen3-VL-4B-Instruct
trust_remote_code: true

# Method
stage: sft
do_train: true
finetuning_type: full

# Data - Update dataset_dir to your data path
dataset: egocross
dataset_dir: ./data
template: qwen3_vl
cutoff_len: 32768
preprocessing_num_workers: 16

# Image/Video Processing (IMPORTANT: prevent OOM)
image_max_pixels: 360000
video_max_pixels: 360000
image_min_pixels: 50176

# Training Hyperparameters
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-5
num_train_epochs: 2
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

# Output
output_dir: ./output/full_sft
logging_steps: 10
save_steps: 999999
save_total_limit: 1

# DeepSpeed (REQUIRED - will OOM without this!)
deepspeed: examples/deepspeed/ds_z2_config.json
